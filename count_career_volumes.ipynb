{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting volumes published during an author's career\n",
    "\n",
    "The goal here is to get a rough sense of how widely an author was purchased -- especially by libraries -- during his or her career. We don't necessarily want to punish authors for dying, so we don't need to mess with death dates. Instead we'll roughly define a \"career\" as a period thirty years before, and after, a \"midcareer\" date â€” which is simply the mean publication date recorded for that author in a set of clean metadata dated as close as possible to first publication.\n",
    "\n",
    "In other words, say our \"clean\" metadata includes four volumes by Thomas Hardy: *Desperate Remedies,* dated to 1874, and three vols of *Tess,* dated to 1891. The mean date will be 1887, so we'll count volumes by Thomas Hardy published between 1857 and 1917. It's not a perfect system, but it's going to capture much of his contemporary popularity, without counting reprints that were driven more by twentieth-century academic fashion. We also set hard limits at 1835 and 1965.\n",
    "\n",
    "The metadata I use is a list of fiction volumes in HathiTrust; since HathiTrust often includes multiple copies of an edition -- but by no means covers every copy in every library -- this ends up being somewhere between a count of *editions* and a count of *individual volumes.*\n",
    "\n",
    "A lot of the effort below goes into capturing name variants. Messy details that have to be handled include pseudonyms and initials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting aliases\n",
      "302\n",
      "343\n",
      "386\n",
      "Done.\n",
      "1040\n"
     ]
    }
   ],
   "source": [
    "# Counting author occurrences is complicated by the Augean stable\n",
    "# that is real-world metadata. Authors' names can be spelled differently,\n",
    "# with initials or names spelled out,\n",
    "# there can be pseudonyms, authors listed under their husband's name, etc.\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def forceint(astring):\n",
    "    try:\n",
    "        intval = int(astring)\n",
    "    except:\n",
    "        intval = 0\n",
    "\n",
    "    return intval\n",
    "\n",
    "def start_the_same(oneauthor, anotherauthor):\n",
    "    if len(oneauthor) < 5 or len(anotherauthor) < 5:\n",
    "        return False\n",
    "    elif oneauthor[0:4].lower() == anotherauthor[0:4].lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# I've got a database of about 1600 works that I've manually cleaned to\n",
    "# be relatively consistent. But now I want to rediscover the mess, in order to\n",
    "# create a dictionary of aliases.\n",
    "\n",
    "aliases = dict()\n",
    "\n",
    "# We get aliases from multiple sources. There is a special pseudonym file:\n",
    "\n",
    "with open('pseudonyms.csv', encoding = 'utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        aliases[row['pseudonym']] = row['ourname']\n",
    "\n",
    "# Another source of aliases is in the clean database itself. I have\n",
    "# a column \"othername\" that records pseudonyms (or real names in cases\n",
    "# where the pseudonym is more common).\n",
    "\n",
    "# While gathering aliases from the clean data we will also \n",
    "# initialize some dictionaries to use later.\n",
    "\n",
    "authorcounts = Counter()\n",
    "# that's where we will ultimately gather our data\n",
    "existingauthors = set()\n",
    "\n",
    "title2name = dict()\n",
    "docid2name = dict()\n",
    "# these mappings will help us generate additional aliases\n",
    "\n",
    "publicationdatesforauth = dict()\n",
    "\n",
    "def harvest_info(df, existingauthors, title2name, docid2name, publicationdatesforauth, aliases):\n",
    "    ''' Gathers information from a single file.\n",
    "    '''\n",
    "    for i in df.index:\n",
    "        mainname = df.loc[i, 'author'].strip(', .')\n",
    "        if '.' in mainname:\n",
    "            spacedout = mainname.replace('.', '. ')\n",
    "            aliases[spacedout] = mainname\n",
    "            condensed = mainname.replace('. ', '.')\n",
    "            aliases[condensed] = mainname\n",
    "            \n",
    "        existingauthors.add(mainname)\n",
    "        othername = str(df.loc[i, 'othername'])\n",
    "\n",
    "        title = df.loc[i,'title'].strip(', .')\n",
    "        docid = df.loc[i, 'docid']\n",
    "\n",
    "        date = forceint(df.loc[i, 'earliestdate'])\n",
    "        if mainname not in publicationdatesforauth:\n",
    "            publicationdatesforauth[mainname] = []\n",
    "        publicationdatesforauth[mainname].append(date)\n",
    "\n",
    "        docid2name[docid] = mainname\n",
    "        if len(title) > 5:\n",
    "            title2name[title] = mainname\n",
    "\n",
    "        if len(othername) < 4:\n",
    "            continue\n",
    "        elif othername == 'Mrs':\n",
    "            othername = mainname + ',' + 'Mrs'\n",
    "            aliases[othername] = mainname.strip()\n",
    "        elif ',' not in othername:\n",
    "            # this is not in lastname, firstnames order yet\n",
    "            words = othername.rpartition(' ')\n",
    "            # that divides only on the *last* space\n",
    "            # e.g.\n",
    "            # 'Edgar Allan Poe'.rpartition(' ')\n",
    "            # Out[13]: ('Edgar Allan', ' ', 'Poe')\n",
    "\n",
    "            if len(words) < 2:\n",
    "                aliases[othername] = mainname\n",
    "            else:\n",
    "                othername = words[1] + \", \" + words[0]\n",
    "                aliases[othername] = mainname\n",
    "        else:\n",
    "            # this is already in right order\n",
    "            aliases[othername] = mainname\n",
    "\n",
    "cleanmeta = pd.read_csv('../fiction/prestigeficmeta.csv')\n",
    "bestsellermeta = pd.read_csv('../sales/bestsellermetadata.csv')\n",
    "\n",
    "harvest_info(cleanmeta, existingauthors, title2name, docid2name, publicationdatesforauth, aliases)\n",
    "harvest_info(bestsellermeta, existingauthors, title2name, docid2name, publicationdatesforauth, aliases)\n",
    "\n",
    "# Now that we have mappings of docids and titles to clean author names, we can use\n",
    "# a bigger messier database to find aliases.\n",
    "\n",
    "# We'll go through a larger database of 100,000 vols, and\n",
    "# every time we find a book that we know is by author X, with a subtly different author\n",
    "# name (Y), we can create an alias: aliases[Y] = X.\n",
    "\n",
    "print('getting aliases')\n",
    "\n",
    "def find_alias(docid, auth, title, docid2name, title2name, aliases, existingauthors):\n",
    "    ''' Using metadata drawn from a single row of a database, decides\n",
    "    whether this author name (auth) should be considered an alias for another\n",
    "    author we had in our clear metadata.\n",
    "    '''\n",
    "    if docid in docid2name and auth not in existingauthors:\n",
    "        if auth != docid2name[docid] and len(auth) > 3:\n",
    "            aliases[auth] = docid2name[docid]\n",
    "\n",
    "    if title in title2name and auth not in existingauthors:\n",
    "        wehadthisbookwrittenby = title2name[title]\n",
    "\n",
    "        # We have a book with a title that also occurred in our\n",
    "        # clean metadata. If it's listed under a variant version\n",
    "        # of the same author, let's create an alias. That means we have to\n",
    "        # ensure, first, that it's not exactly the same\n",
    "\n",
    "        if auth == wehadthisbookwrittenby:\n",
    "            # the book is listed under the same name in the clean metadata\n",
    "            # no alias needed\n",
    "            return\n",
    "\n",
    "        # and second, that it's close enough to be the same person\n",
    "        if start_the_same(auth, wehadthisbookwrittenby):\n",
    "            aliases[auth] = wehadthisbookwrittenby\n",
    "            \n",
    "print(len(aliases))\n",
    "            \n",
    "with open('/Users/tunder/work/genre/metadata/ficmeta.csv', encoding = 'latin-1') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        docid = row['htid']\n",
    "        auth = row['author'].strip(', .')\n",
    "        title = row['title'].split('|')[0].strip(', .')\n",
    "        find_alias(docid, auth, title, docid2name, title2name, aliases, existingauthors)\n",
    "        \n",
    "print(len(aliases))\n",
    "        \n",
    "with open('/Users/tunder/Dropbox/python/train20/subfiction/filteredfiction.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        date = int(row['inferreddate'])\n",
    "        if date > 1959:\n",
    "            continue\n",
    "        docid = row['docid']\n",
    "        auth = row['author'].strip(', .')\n",
    "        title = row['title'].split('|')[0].strip(', .')\n",
    "        find_alias(docid, auth, title, docid2name, title2name, aliases, existingauthors)\n",
    "        \n",
    "print(len(aliases))\n",
    "\n",
    "# Now we're going to actually count authors' works in univ. libraries.\n",
    "# But we're only going to do that for a 50-year span we call the author's\n",
    "# career.\n",
    "\n",
    "careers = dict()\n",
    "\n",
    "for authorname, pubdates in publicationdatesforauth.items():\n",
    "    if len(pubdates) < 1:\n",
    "        print(\"that shouldn't happen\")\n",
    "    thiscareer = dict()\n",
    "    thiscareer['midpoint'] = sum(pubdates) / len(pubdates)\n",
    "    thiscareer['start'] = thiscareer['midpoint'] - 30\n",
    "    thiscareer['end'] = thiscareer['midpoint'] + 31\n",
    "    \n",
    "    # But we only allow careers to go ten years beyond our\n",
    "    # timeline endpoints. We're placing this limit to echo the\n",
    "    # constraints on our bestseller lists.\n",
    "    \n",
    "    if thiscareer['start'] < 1835:\n",
    "        thiscareer['start'] = 1835\n",
    "    if thiscareer['end'] > 1965:\n",
    "        thiscareer['end'] = 1965\n",
    "    \n",
    "    careers[authorname] = thiscareer\n",
    "\n",
    "# okay, now we can count references\n",
    "\n",
    "# I know I'm not being super-efficient about disk access, but\n",
    "# this is not a frequently-run process, and premature\n",
    "# optimization is the root of all evil.\n",
    "\n",
    "def checkrow(auth, date, existingauthors, careers, authorcounts, aliases):\n",
    "    if auth in existingauthors:\n",
    "        if date >= careers[auth]['start'] and date < careers[auth]['end']:\n",
    "            authorcounts[auth] += 1\n",
    "    elif auth in aliases:\n",
    "        correct_author = aliases[auth]\n",
    "        if date >= careers[correct_author]['start'] and date < careers[correct_author]['end']:\n",
    "            authorcounts[correct_author] += 1\n",
    "\n",
    "with open('/Users/tunder/work/genre/metadata/ficmeta.csv', encoding = 'latin-1') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        date = forceint(row['startdate'])\n",
    "        auth = row['author'].strip(', .')\n",
    "        checkrow(auth, date, existingauthors, careers, authorcounts, aliases)\n",
    "\n",
    "with open('/Users/tunder/Dropbox/python/train20/subfiction/filteredfiction.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        date = forceint(row['inferreddate'])\n",
    "        auth = row['author'].strip(',. ')\n",
    "        checkrow(auth, date, existingauthors, careers, authorcounts, aliases)\n",
    "\n",
    "with open('/Users/tunder/Dropbox/pulp/pulpstories.csv', encoding = 'latin-1') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        date = forceint(row['magpubdate'])\n",
    "        auth = row['authorname']\n",
    "        checkrow(auth, date, existingauthors, careers, authorcounts, aliases)\n",
    "\n",
    "with open('/Users/tunder/Dropbox/pulp/novelsupplement.csv', encoding = 'latin-1') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        date = forceint(row['serialend'])\n",
    "        auth = row['authorname']\n",
    "        checkrow(auth, date, existingauthors, careers, authorcounts, aliases)\n",
    "    \n",
    "print('Done.')\n",
    "print(len(authorcounts))\n",
    "with open('career_volumes.csv', mode = 'w', encoding = 'utf-8') as f:\n",
    "    scribe = csv.DictWriter(f, fieldnames = ['author', 'raw_num_vols', 'midcareer'])\n",
    "    scribe.writeheader()\n",
    "    for author in existingauthors:\n",
    "        if author == '<blank>':\n",
    "            continue\n",
    "        thisrow = dict()\n",
    "        thisrow['author'] = author\n",
    "        thisrow['raw_num_vols'] = authorcounts[author]\n",
    "        thisrow['midcareer'] = int(careers[author]['midpoint'])\n",
    "        scribe.writerow(thisrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
